{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7ef0d1-0e5e-4dd6-ba5b-b6d530393946",
   "metadata": {},
   "source": [
    "# About Ray\n",
    "\n",
    "<img src='images/ray_header_logo.png' width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39f2fa-2181-4402-921c-cb0a3cec7357",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "#### What is Ray?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://www.ray.io/\" target=\"_blank\">Ray</a></strong> is an open-source unified compute framework that makes it easy to scale AI and Python workloads.\n",
    "</div>\n",
    "\n",
    "Thanks to the Python first approach, ML engineers can parallelize Python applications on their laptop, cluster, cloud, Kubernetes, or on-premise hardware. Ray automatically handles all aspects of distributed execution including orchestration, scheduling, fault tolerance, and auto-scaling so that you can scale your apps without becoming a distributed systems expert.\n",
    "\n",
    "With a rich ecosystem of libraries and integrations with many important data science tools, Ray lowers the effort needed to scale compute intensive workloads and applications.\n",
    "\n",
    "#### Distributed computing: a bit of a context and project history\n",
    "\n",
    "|<img src=\"images/project_history.jpeg\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Compute demand is growing faster than supply. It exceeds progression of CPUs, GPUs and TPUs processing power. (date accessed: Nov 2, 2022)|\n",
    "\n",
    "Distributed computing is hard. At the same time it is becoming increasingly crucial and necessary for modern machine learning and AI systems.\n",
    "\n",
    "OpenAI's recent paper [AI and Compute](https://openai.com/blog/ai-and-compute/) suggests exponential growth in compute needed to train AI models. Study suggests that compute needed for AI systems has been doubling every 3.4 months since 2012.\n",
    "\n",
    "This context drove researchers to begin building solutions to simplify running code on compute clusters without having to think about how to orchestrate and utilize individual machines. That is, let Ray do the hard bit of orchestrating and executing, and you do the easy bit of writing Python code.\n",
    "\n",
    "Ray was developed at the University of California Berkeley's [RISELab](https://rise.cs.berkeley.edu/), the successor to the [AMPLab](https://amplab.cs.berkeley.edu/about/), that created [Apache Spark](https://spark.apache.org/) and [Mesos](https://mesos.apache.org/). \n",
    "\n",
    "[Anyscale](https://www.anyscale.com/), the company behind Ray, was founded by Ray creators to build a managed Ray platform and offers hosted solutions for Ray applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88a8ca-21f0-4b7f-b855-bf9cf74d9031",
   "metadata": {},
   "source": [
    "### Key Ray characteristics\n",
    "\n",
    "|<img src=\"images/python_first.jpeg\" width=\"70%\" loading=\"lazy\">|<img src=\"images/simple_and_flexible_api.jpeg\" width=\"70%\" loading=\"lazy\">|<img src=\"images/scalability.jpeg\" width=\"70%\" loading=\"lazy\">|<img src=\"images/heterogeneous_hardware.jpeg\" width=\"70%\" loading=\"lazy\">|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|Python first approach|Simple and Flexible API|Scalability|Support for heterogeneous hardware|\n",
    "\n",
    "#### Python first approach\n",
    "\n",
    "<img src=\"images/python_first.jpeg\" width=\"100px\" loading=\"lazy\">\n",
    "\n",
    "Ray's framework provides Python library with abstractions and primitives that enables ML practitioners and Python developers to build distributed applications. Ray exposes concise and easy to use API. Its core library that enables parallel execution introduces just a few key abstractions:\n",
    "\n",
    "1. [Tasks](https://docs.ray.io/en/latest/ray-core/key-concepts.html#tasks): remote, stateless Python functions\n",
    "1. [Actors](https://docs.ray.io/en/latest/ray-core/key-concepts.html#actors): remote stateful Python classes\n",
    "1. [Objects](https://docs.ray.io/en/latest/ray-core/key-concepts.html#objects): in-memory, immutable objects or value that can be accessed anywhere in the computing cluster\n",
    "\n",
    "#### Simple and flexible API\n",
    "\n",
    "<img src=\"images/simple_and_flexible_api.jpeg\" width=\"100px\" loading=\"lazy\">\n",
    "\n",
    "##### Ray Core\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-core/walkthrough.html\" target=\"_blank\">Ray Core</a></strong> is an open-source, Python, general purpose, distributed computing library that enables ML engineers and Python developers to scale Python applications and accelerate machine learning workloads.\n",
    "</div>\n",
    "\n",
    "Foundational library for the whole ecosystem - provides minimalist API that enables distributed computing. With just a few methods you can start building distributed apps.\n",
    "\n",
    "* `ray.init()` - start Ray runtime and connect to the Ray cluster\n",
    "* `@ray.remote` -  functions and classes decorator specifying that it will be executed as a task (remote function) or actor (remote class) in a different process\n",
    "* `.remote` - postfix to the remote functions and classes. Remote operations are *asynchronous*\n",
    "* `ray.put()` - put an object in the in-memory object store and return its ID. Use this ID to pass object to any remote function or method call\n",
    "* `ray.get()` - get a remote object or a list of remote objects from the object store\n",
    "\n",
    "##### Ray AI Runtime (AIR)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-air/getting-started.html\" target=\"_blank\">Ray AI Runtime (AIR)</a></strong> is an open-source, Python, domain specific library that equips ML engineers, data scientists, and researchers with a scalable and unified toolkit for ML applications.\n",
    "</div>\n",
    "\n",
    "Ray AI Runtime (AIR) (sometimes referred to as native libraries) and ecosystem libraries, provides higher level APIs that cater for more domain specific use cases. Ray AIR enables Python developers and ML engineers to scale individual workloads, end-to-end workflows, and popular ecosystem frameworks, all in familiar Python programming language.\n",
    "\n",
    "#### Scalability\n",
    "\n",
    "<img src=\"images/scalability.jpeg\" width=\"100px\" loading=\"lazy\">\n",
    "\n",
    "Ray allows their users to utilize large compute clusters in an easy, productive, and resource-efficient way.\n",
    "\n",
    "Fundamentally, Ray treats the entire cluster as a single, unified pool of resources and takes care of optimally mapping compute workloads to the pool. By doing so, Ray largely eliminates non-scalable factors in the system. Successful user stories include, but are not limited to:\n",
    "* [how Instacart uses Ray to power their large scale fulfillment ML pipline](https://www.youtube.com/watch?v=3t26ucTy0Rs),\n",
    "* [how OpenAI trains their largest models](https://twitter.com/anyscalecompute/status/1562136159135973380),\n",
    "* [how companies like HuggingFace and Cohere use Ray Train for scaling model training](https://www.youtube.com/watch?v=For8yLkZP5w).\n",
    "\n",
    "Ray's [autoscaler](https://docs.ray.io/en/latest/cluster/key-concepts.html#autoscaling) implements automatic scaling of Ray clusters based on the resource demands of an application. The autoscaler will increase worker nodes when the Ray workload exceeds the cluster's capacity. Whenever worker nodes sit idle, the autoscaler will scale them down.\n",
    "\n",
    "#### Support for heterogeneous hardware\n",
    "\n",
    "<img src=\"images/heterogeneous_hardware.jpeg\" width=\"100px\" loading=\"lazy\">\n",
    "\n",
    "One of the key properties of Ray is natively supporting heterogeneous hardware by allowing developers to specify such hardware when instantiating a Task or Actor. For example, a developer can specify in the same application that one Task needs 128 CPUs, while an Actor requires 36 CPUs and 8 Nvidia A100 GPUs.\n",
    "\n",
    "An illustrative example is the [production deep learning pipeline at Uber](https://www.anyscale.com/ray-summit-2022/agenda/sessions/215). A heterogeneous setup of 8 GPU nodes and 9 CPU nodes improves the pipeline throughput by 50%, while substantially saving capital cost, compared with the legacy setup of 16 GPU nodes.\n",
    "\n",
    "|<img src=\"images/uber.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Production deep learning pipeline at Uber.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44801c40-8826-4845-aa3e-a53911d7245b",
   "metadata": {},
   "source": [
    "Let's jump in and take a very quick look at code to\n",
    "* access data\n",
    "* train a (distributed) XGBoost model\n",
    "* serve that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38d360-65eb-4f58-84d7-f0c874850afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae3a3d-839b-412b-9bfb-ceed50723bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ray.data.read_parquet(\"s3://anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet\")\n",
    "\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "train_dataset = train_dataset.repartition(num_blocks=5)\n",
    "valid_dataset = valid_dataset.repartition(num_blocks=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f74ef66-9aeb-4562-857b-244fcda546d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"is_big_tip\",\n",
    "    num_boost_round=20,\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=False),\n",
    "    params={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "        \"tree_method\": \"approx\",\n",
    "    },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a5403-cb93-4e1a-b942-76656314268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212d31a-a698-44ae-ba54-5973101f2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89bd8a-faba-48bf-9e95-bdc43817cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.serve.http_adapters import pandas_read_json\n",
    "from ray.train.xgboost import XGBoostPredictor\n",
    "\n",
    "serve.run(\n",
    "    PredictorDeployment.options(\n",
    "        name=\"XGBoostService\", num_replicas=2, route_prefix=\"/rayair\"\n",
    "    ).bind(XGBoostPredictor, result.checkpoint, http_adapter=pandas_read_json)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa07e6a-9787-432d-87c9-669d6f6b1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "sample_input = train_dataset.take(1)\n",
    "\n",
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d0351-e115-426d-9ada-54795750efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = dict(sample_input[0])\n",
    "del(sample_input['is_big_tip'])\n",
    "del(sample_input['__index_level_0__'])\n",
    "\n",
    "output = requests.post(\"http://localhost:8000/rayair\", json=[sample_input]).json()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9cd090-04d5-4c81-bce7-ad2515a71487",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5217e938-2edd-40c6-b147-9eb6739b0780",
   "metadata": {
    "tags": []
   },
   "source": [
    "|<img src=\"images/map.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Stack of Ray libraries - unified toolkit for ML workloads.|\n",
    "\n",
    "#### Ray AI Runtime\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-air/getting-started.html\" target=\"_blank\">Ray AI Runtime (AIR)</a></strong> is an open-source, Python, domain specific library that equips ML engineers, data scientists, and researchers with a scalable and unified toolkit for ML applications.\n",
    "</div>\n",
    "\n",
    "Ray AIR is built on top of Ray core. It caters for distributed data processing, model training, tuning, model serving, and reinforcement learning, all in Python. To that end, it enables both individual workloads and end-to-end use cases to be implemented in the single unified library.\n",
    "\n",
    "Ray AIR brings together an ever-growing ecosystem of integrations with your favorite machine learning frameworks.\n",
    "\n",
    "|<img src=\"images/e2e_air.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR enables end-to-end ML development and provides multiple options to integrate with other tools and libraries form the MLOps ecosystem.|\n",
    "\n",
    "Each of the five native libraries that Ray AIR wraps is focused on a specific ML task. Because this abstraction layer is built on top of Ray Core, it is distributed and scalable.\n",
    "\n",
    "1. [Ray Data](https://docs.ray.io/en/latest/data/dataset.html): scalable, framework-agnostic loading and transforming raw data across training and prediction\n",
    "1. [Ray Train](https://docs.ray.io/en/latest/train/train.html): distributed multi-node and multi-core model training with fault tolerance that integrates with your favorite training libraries\n",
    "1. [Ray Tune](https://docs.ray.io/en/latest/tune/index.html): scales experiment execution and hyperparameter tuning to optimize model performance\n",
    "1. [Ray Serve](https://docs.ray.io/en/latest/serve/index.html): deploys your model for online inference, with optional microbatching to improve performance\n",
    "1. [Ray RLlib](https://docs.ray.io/en/latest/rllib/index.html): distributed reinforcement learning workloads that integrate with the other Ray AIR libraries above\n",
    "\n",
    "#### Integrations and ecosystem libraries\n",
    "\n",
    "Ray integrates with a [growing ecosystem](https://docs.ray.io/en/latest/ray-overview/ray-libraries.html) of the most popular Python and machine learning libraries and frameworks that you may already be familiar with. Instead of trying to create new standards, Ray allows you to scale existing workloads by unifying tools in a common interface. This interface enables you to run ML tasks in a distributed way, a property most of the respective backends don't have, or not to the same extent.\n",
    "\n",
    "For example, Ray Datasets is backed by Arrow and comes with many integrations to other frameworks, such as Spark and Dask. Ray Train and RLlib are backed by the full power of Tensorflow and PyTorch. Ray Tune supports algorithms from practically every noteable HPO tool available, including Hyperopt, Optuna, Nevergrad, Ax, SigOpt, and many others. Ray Serve can be used with frameworks such as FastAPI, gradio, and Streamlit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
