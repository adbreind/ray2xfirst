{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RLlib\n",
    "\n",
    "> __Reinforcement Learning__ is a family of techniques that train *agents* to act in an *environment* to maximize *reward*. Famous examples include agents that can play chess, go, or Atari games ... but the field is hot because those agents can also be robots learning to do work, autonomous vehicles driving, or even virtual salesmen learning to get the best price possible from a customer.\n",
    "\n",
    "Ray provides RLlib as a high-level library that encapsulates both the distribution (clustered) training as well as many popular reinforcement learning algorithms (the code that turns interactions and rewards into models and policies).\n",
    "\n",
    "Here, to create a simple example, we'll use __Deep Q-Learning__ (a foundational deep RL algorithm) to learn OpenAI's \"cart-pole\" (https://gym.openai.com/envs/CartPole-v1/) environment, which you can visualize like this:\n",
    "\n",
    "<video src='images/cpv1.mp4' controls='true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example, and the lab, are based on the demo in Dean Wampler's excellent intro paper, \"What is Ray?\" on O'Reilly Safari Online: https://learning.oreilly.com/library/view/what-is-ray/9781492085768/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "config = (  # 1. Configure the algorithm,\n",
    "    DQNConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .rollouts(num_rollout_workers=2)\n",
    "    .framework(\"torch\")\n",
    "    .training(model={\"fcnet_hiddens\": [64, 64]})\n",
    "    .evaluation(evaluation_num_workers=1)\n",
    ")\n",
    "\n",
    "algo = config.build()  # 2. build the algorithm,\n",
    "\n",
    "#for _ in range(5):\n",
    "print(algo.train())  # 3. train it,\n",
    "\n",
    "algo.evaluate()  # 4. and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = '{:3d},{:8.4f},{:8.4f},{:8.4f}'\n",
    "last_checkpoint = ''\n",
    "for n in range(N_ITER):\n",
    "  #  result = trainer.train()\n",
    "    min  = result['episode_reward_min']\n",
    "    mean = result['episode_reward_mean']\n",
    "    max  = result['episode_reward_max']\n",
    "    last_checkpoint = trainer.save(checkpoint_dir)\n",
    "    print(fmt.format(n, min, mean, max))\n",
    "print(f'last checkpoint file: {last_checkpoint}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: If you've worked with RL and OpenAI gym before, you may realize these are not particularly impressive numbers, and not a particularly impressive algorithm.\n",
    "\n",
    "Don't worry: __Ray RLlib__ includes a variety of more powerful algorithms which achieve better results. We'll try one of them -- Proximal Policy Optimization (PPO) in the lab exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib Usage\n",
    "\n",
    "How you approach RLlib will depend on whether you are toward the research direction vs. the applied direction in your current work.\n",
    "\n",
    "Today, we're focused on getting started with applying RLlib using its built-in architectural patterns and algorithms, but customizing\n",
    "* the environment (environment representation, action choices, rewards)\n",
    "* (optionally) the model\n",
    "\n",
    "Customization options are documented at https://docs.ray.io/en/master/rllib-env.html -- we'll start with the simplest custom environment integration to get started. This is based on an example in the Ray source examples.\n",
    "\n",
    "In this scenario, we have\n",
    "* simple, linear corridor of locations (states) where the agent starts at one end and gets a reward if/when it reaches the other end\n",
    "* two possible actions: forward (away from starting state) or backward\n",
    "* random positive reward upon reaching the goal; small loss (negative reward) for actions that to not reach the goal\n",
    "\n",
    "The simplest way to start with a custom environment is to subclass the OpenAI `gym.Env` class, as this protocol is natively supported by Ray. The base class documentation is at https://github.com/openai/gym/blob/master/gym/core.py and there is some intro documentation at https://gym.openai.com/docs/#getting-started-with-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Example of a custom env in which you have to walk down a corridor.\n",
    "    You can configure the length of the corridor via the env config.\"\"\"\n",
    "\n",
    "    def __init__(self, config: EnvContext):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = Discrete(2)\n",
    "        self.observation_space = Box(\n",
    "            0.0, self.end_pos, shape=(1, ), dtype=np.float64)\n",
    "        # Set the seed. This is only used for the final (reach goal) reward.\n",
    "        self.seed(config.worker_index * config.num_workers)\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos = 0\n",
    "        return [self.cur_pos]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in [0, 1], action\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "        done = self.cur_pos >= self.end_pos\n",
    "        # Produce a random reward when we reach the goal.\n",
    "        return [self.cur_pos], \\\n",
    "            random.random() * 2 if done else -0.1, done, {}\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous example, we used a shorthand configuration for the neural net model, specifying the dimenstions of a fully-connected (multilayer perceptron style) network. \n",
    "\n",
    "Here, we'll show how to provide a custom model. To keep things simple, this custom model actually just delegates to the same fully-connected network helper code, but the important thing is it shows the \"integration glue\" for hooking in any custom model, based on the standard PyTorch `nn.Module` pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCustomModel(TorchModelV2, nn.Module):\n",
    "    \"\"\"Example of a PyTorch custom model that just delegates to a fc-net.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.torch_sub_model = TorchFC(obs_space, action_space, num_outputs,\n",
    "                                       model_config, name)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        input_dict[\"obs\"] = input_dict[\"obs\"].float()\n",
    "        fc_out, _ = self.torch_sub_model(input_dict, state, seq_lens)\n",
    "        return fc_out, []\n",
    "\n",
    "    def value_function(self):\n",
    "        return torch.reshape(self.torch_sub_model.value_function(), [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an environment and a model, we're ready to have RLlib train an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dqn import DQN\n",
    "\n",
    "ModelCatalog.register_custom_model(\"my_model\", TorchCustomModel)\n",
    "\n",
    "algo = DQN.get_default_config().environment(SimpleCorridor, env_config={'corridor_length':5}) \\\n",
    "                               .framework('torch') \\\n",
    "                               .training(model={\"custom_model\": \"my_model\", \"vf_share_layers\": True,}) \\\n",
    "                               .build()\n",
    "\n",
    "'''\n",
    "for n in range(20): #training iterations\n",
    "    result = trainer.train()\n",
    "    min  = result['episode_reward_min']\n",
    "    mean = result['episode_reward_mean']\n",
    "    max  = result['episode_reward_max']\n",
    "    fmt = '{:3d},{:8.4f},{:8.4f},{:8.4f}'\n",
    "    print(fmt.format(n, min, mean, max))\n",
    "    \n",
    "print(pretty_print(result))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Ray RLlib and PPO\n",
    "\n",
    "PPO of Proximal Policy Optimization is a more powerful (and more complicated) algorithm than the DQN we've looked at.\n",
    "\n",
    "But thanks to Ray's implementations, you can swap it in easily.\n",
    "\n",
    "__We'll redo the earlier OpenAI Gym Cart-Pole problem, but swap in PPO for the algorithm__\n",
    "\n",
    "Note that we import `ppo` from `ray.rllib.agents`\n",
    "\n",
    "By replacing \"DQN\" with \"PPO\" you can quickly get better results.\n",
    "\n",
    ">\n",
    "> Interested in PPO details? Check out this writeup: https://jonathan-hui.medium.com/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the code from the OpenAI gym example, but replace references to DQN with references to PPO\n",
    "\n",
    "# HINT: try 10 iterations -- that will be plenty for PPO to solve the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "Of course, getting an agent to walk down a 5-step corridor is not the same as solving a complex business or research problem.\n",
    "\n",
    "The goal today is to\n",
    "* explain how RLlib integrates the various ingredients of a distributed deep reinforcement learning system\n",
    "* enable you to take the first steps toward specifying your own environment(s) and seeing how well the agent can (or can't) learn\n",
    "\n",
    "__Next steps__\n",
    "\n",
    "There is a good walkthrough of RLlib environment integration starting with the basics at https://medium.com/distributed-computing-with-ray/anatomy-of-a-custom-environment-for-rllib-327157f269e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
