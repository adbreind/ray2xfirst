{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ebd169-cbf4-44dc-b863-0aa147b8050e",
   "metadata": {},
   "source": [
    "# Scalable, Flexible Serving with Ray Serve\n",
    "\n",
    "<img src='images/servelogo.svg' width=400>\n",
    "\n",
    "Earlier, we saw a server that performed simple request/response operation.\n",
    "\n",
    "While it's nice to have that feature without deploying additional software, that pattern is fairly well understood and easily scaled with existing technology.\n",
    "\n",
    "Ray Serve provides more value as we move to more complex patterns such as\n",
    "- stateful services\n",
    "- batching\n",
    "- composition\n",
    "- integration to model registries\n",
    "\n",
    "Let's start with a simple service deployment that could work with just about any model or logic. This will also help \"take some of the magic away\" from that 3-line serve demo featuring a `XGBoostPredictor` and `PredictorDeployment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65198e11-3d2f-49db-a682-a205e7af8529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-27 09:51:25,742\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=76357)\u001b[0m INFO 2022-12-27 09:51:27,827 controller 76357 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-8f2863c93327a10a49eab8494bb169b1fc40c2a17807c4534e395f6d' on node '8f2863c93327a10a49eab8494bb169b1fc40c2a17807c4534e395f6d' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO:     Started server process [76360]\n",
      "\u001b[2m\u001b[36m(ServeController pid=76357)\u001b[0m INFO 2022-12-27 09:51:28,857 controller 76357 deployment_state.py:1310 - Adding 2 replicas to deployment 'GenericDeployment'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='GenericDeployment')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from starlette.requests import Request\n",
    "from typing import Dict\n",
    "import json\n",
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "class MyModel:\n",
    "    def __init__(self, demo_param: int):\n",
    "        self._demo_param = demo_param\n",
    "        \n",
    "    def predict(self, data):\n",
    "        return data * self._demo_param\n",
    "\n",
    "@serve.deployment(route_prefix=\"/\", num_replicas=2)\n",
    "class GenericDeployment:\n",
    "    def __init__(self, demo_param:int):        \n",
    "        self._model = MyModel(demo_param)\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        return { \"result\" : self._model.predict(data['input']) }\n",
    "\n",
    "serve.run(GenericDeployment.bind(demo_param=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55178764-dd56-42f7-8857-d215cee9a686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"input\":7}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_json = '{\"input\":7}'\n",
    "sample_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6ad7074-90a0-4c88-a888-fb2b2d8320a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 294}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:51:34,599 http_proxy 127.0.0.1 http_proxy.py:361 - POST / 200 4.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:GenericDeployment pid=76362)\u001b[0m INFO 2022-12-27 09:51:34,598 GenericDeployment GenericDeployment#kToazM replica.py:505 - HANDLE __call__ OK 0.2ms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "print(requests.post(\"http://localhost:8000/\", json = sample_json).json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0acd8-3817-4ffb-b542-59cc7ee245b5",
   "metadata": {},
   "source": [
    "Ok that illustrates the framework pattern a little bit.\n",
    "\n",
    "Next let's look at some more interesting features that can power more complex use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "292127ed-d10c-43f6-837b-f1beafb30779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=76357)\u001b[0m INFO 2022-12-27 09:52:38,044 controller 76357 deployment_state.py:1310 - Adding 1 replica to deployment 'Counter'.\n"
     ]
    }
   ],
   "source": [
    "@serve.deployment\n",
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        self.count += 1\n",
    "        return {\"count\": self.count}\n",
    "\n",
    "Counter.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a372ed0-0a11-4084-b02d-365d2f9896a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:53:10,139 http_proxy 127.0.0.1 http_proxy.py:361 - GET /Counter 200 3.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:Counter pid=76371)\u001b[0m INFO 2022-12-27 09:53:10,137 Counter Counter#SoYBaz replica.py:505 - HANDLE __call__ OK 0.1ms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.get(\"http://127.0.0.1:8000/Counter\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e397944-3926-4c62-95b1-a0de1546afd9",
   "metadata": {},
   "source": [
    "We can also invoke these services directly from other applications or services in our Ray application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a805de54-6397-4eaf-8655-c44090c90b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:Counter pid=76371)\u001b[0m INFO 2022-12-27 09:53:43,638 Counter Counter#SoYBaz replica.py:505 - HANDLE __call__ OK 0.1ms\n"
     ]
    }
   ],
   "source": [
    "ray.get(Counter.get_handle().remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12224fbb-1cbb-4acb-b7da-a261ae7d8d87",
   "metadata": {},
   "source": [
    "Many models achieve much better per-record performance when evaluating batches of records.\n",
    "\n",
    "We can use Ray Serve to build that batching layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eee28cbb-65ae-4ff4-ba05-61d49911ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "@serve.deployment(route_prefix=\"/adder\")\n",
    "class BatchAdder:\n",
    "    @serve.batch(max_batch_size=4)\n",
    "    async def handle_batch(self, numbers):\n",
    "        input_array = np.array(numbers)\n",
    "        print(\"Our input array has shape:\", input_array.shape)\n",
    "        # Sleep for 200ms, this could be performing CPU intensive computation\n",
    "        # in real models\n",
    "        time.sleep(0.2)\n",
    "        output_array = input_array + 1\n",
    "        return output_array.astype(int).tolist()\n",
    "\n",
    "    async def __call__(self, request):\n",
    "        return await self.handle_batch(int(request.query_params[\"number\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11519947-390b-4512-842f-4678a4c9c584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=76357)\u001b[0m INFO 2022-12-27 09:53:58,649 controller 76357 deployment_state.py:1310 - Adding 1 replica to deployment 'BatchAdder'.\n"
     ]
    }
   ],
   "source": [
    "BatchAdder.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77fc5601-b4c9-4112-b4f9-919f2b045937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(i):\n",
    "    return requests.get(\"http://localhost:8000/adder?number={}\".format(i)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c658155-700b-4e53-8353-3b228b1d71c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m Our input array has shape: (1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'18'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:54:04,015 http_proxy 127.0.0.1 http_proxy.py:361 - GET /adder 200 209.7ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m INFO 2022-12-27 09:54:04,013 BatchAdder BatchAdder#LKmjvF replica.py:505 - HANDLE __call__ OK 204.2ms\n"
     ]
    }
   ],
   "source": [
    "make_request(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a411ab2-4f1c-46e5-8572-d71948e4d8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m Our input array has shape: (1,)\n",
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m Our input array has shape: (1,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m INFO 2022-12-27 09:54:08,075 BatchAdder BatchAdder#LKmjvF replica.py:505 - HANDLE __call__ OK 201.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:54:08,283 http_proxy 127.0.0.1 http_proxy.py:361 - GET /adder 200 412.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:54:08,285 http_proxy 127.0.0.1 http_proxy.py:361 - GET /adder 200 413.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m INFO 2022-12-27 09:54:08,281 BatchAdder BatchAdder#LKmjvF replica.py:505 - HANDLE __call__ OK 206.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m Our input array has shape: (1,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:54:08,491 http_proxy 127.0.0.1 http_proxy.py:361 - GET /adder 200 618.5ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m INFO 2022-12-27 09:54:08,488 BatchAdder BatchAdder#LKmjvF replica.py:505 - HANDLE __call__ OK 203.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m Our input array has shape: (4,)\n",
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m Our input array has shape: (3,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m INFO 2022-12-27 09:54:08,897 BatchAdder BatchAdder#LKmjvF replica.py:505 - HANDLE __call__ OK 408.9ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m INFO 2022-12-27 09:54:08,898 BatchAdder BatchAdder#LKmjvF replica.py:505 - HANDLE __call__ OK 409.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m INFO 2022-12-27 09:54:08,898 BatchAdder BatchAdder#LKmjvF replica.py:505 - HANDLE __call__ OK 409.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m INFO 2022-12-27 09:54:08,898 BatchAdder BatchAdder#LKmjvF replica.py:505 - HANDLE __call__ OK 409.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m INFO 2022-12-27 09:54:08,898 BatchAdder BatchAdder#LKmjvF replica.py:505 - HANDLE __call__ OK 409.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m INFO 2022-12-27 09:54:08,898 BatchAdder BatchAdder#LKmjvF replica.py:505 - HANDLE __call__ OK 409.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:BatchAdder pid=76384)\u001b[0m INFO 2022-12-27 09:54:08,899 BatchAdder BatchAdder#LKmjvF replica.py:505 - HANDLE __call__ OK 409.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:54:08,902 http_proxy 127.0.0.1 http_proxy.py:361 - GET /adder 200 1027.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:54:08,903 http_proxy 127.0.0.1 http_proxy.py:361 - GET /adder 200 1027.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:54:08,903 http_proxy 127.0.0.1 http_proxy.py:361 - GET /adder 200 1026.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:54:08,904 http_proxy 127.0.0.1 http_proxy.py:361 - GET /adder 200 1026.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:54:08,905 http_proxy 127.0.0.1 http_proxy.py:361 - GET /adder 200 1026.6ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:54:08,905 http_proxy 127.0.0.1 http_proxy.py:361 - GET /adder 200 1027.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:54:08,906 http_proxy 127.0.0.1 http_proxy.py:361 - GET /adder 200 1025.8ms\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "executor = ThreadPoolExecutor()\n",
    "\n",
    "results = executor.map(make_request, range(0, 20, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d8dc01d-4986-4a53-921e-587a8db7cc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '3', '5', '7', '9', '11', '13', '15', '17', '19']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de24af-9b7f-409c-bfe3-8271d551019b",
   "metadata": {},
   "source": [
    "__Featurization/Model Composition__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef046d-c41d-4453-b7f6-c5b941c5cc4c",
   "metadata": {},
   "source": [
    "Our pipeline will be structured as follows:\n",
    "- Input comes in, the composed model sends it to `model_one`\n",
    "- `model_one` outputs a random number between 0 and 1, if the value is\n",
    "  greater than 0.5, then the data is sent to `model_two`\n",
    "- otherwise, the data is returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a857dae9-3d3c-4028-88bf-ce578c885f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=76357)\u001b[0m INFO 2022-12-27 09:54:32,436 controller 76357 deployment_state.py:1310 - Adding 1 replica to deployment 'model_one'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=76357)\u001b[0m INFO 2022-12-27 09:54:34,419 controller 76357 deployment_state.py:1310 - Adding 1 replica to deployment 'model_two'.\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "@serve.deployment(route_prefix='/one') # remove /refactor this (route) for stateless?\n",
    "def model_one(data):\n",
    "    print(\"Model 1 called with data \", data)\n",
    "    return random()\n",
    "\n",
    "model_one.deploy()\n",
    "\n",
    "@serve.deployment\n",
    "def model_two(data):\n",
    "    print(\"Model 2 called with data \", data)\n",
    "    return data\n",
    "\n",
    "model_two.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82229466-d5e2-4e79-a390-17250df47b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.822915679518294"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:54:37,853 http_proxy 127.0.0.1 http_proxy.py:361 - GET /one 200 5.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_one pid=76390)\u001b[0m INFO 2022-12-27 09:54:37,851 model_one model_one#LQThHy replica.py:505 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:model_one pid=76390)\u001b[0m Model 1 called with data  <starlette.requests.Request object at 0x7fbfd8a24df0>\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get(\"http://127.0.0.1:8000/one\", data=\"hey!\") # stateless demo only\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7c49a93-b646-4663-ac69-b88120e56233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=76357)\u001b[0m INFO 2022-12-27 09:55:02,462 controller 76357 deployment_state.py:1310 - Adding 1 replica to deployment 'ComposedModel'.\n"
     ]
    }
   ],
   "source": [
    "# max_concurrent_queries is optional. By default, if you pass in an async\n",
    "# function, Ray Serve sets the limit to a high number.\n",
    "@serve.deployment(max_concurrent_queries=10, route_prefix=\"/composed\")\n",
    "class ComposedModel:\n",
    "    def __init__(self):\n",
    "        self.model_one = model_one.get_handle()\n",
    "        self.model_two = model_two.get_handle()\n",
    "\n",
    "    # This method can be called concurrently!\n",
    "    async def __call__(self, starlette_request):\n",
    "        data = await starlette_request.body()\n",
    "\n",
    "        score = await self.model_one.remote(data=data)\n",
    "        if score > 0.5:\n",
    "            result = await self.model_two.remote(data=data)\n",
    "            result = {\"model_used\": 2, \"score\": score}\n",
    "        else:\n",
    "            result = {\"model_used\": 1, \"score\": score}\n",
    "\n",
    "        return result\n",
    "\n",
    "ComposedModel.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c184de3e-b1f7-408f-91cc-38684a63dc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_used': 1, 'score': 0.4401545829688882}\n",
      "{'model_used': 1, 'score': 0.03534013582644946}\n",
      "{'model_used': 2, 'score': 0.7653627795826481}\n",
      "{'model_used': 1, 'score': 0.42902257419633383}\n",
      "{'model_used': 1, 'score': 0.14861545856688863}\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_one pid=76390)\u001b[0m Model 1 called with data  b'hey!'\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_one pid=76390)\u001b[0m Model 1 called with data  b'hey!'\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_one pid=76390)\u001b[0m Model 1 called with data  b'hey!'\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_one pid=76390)\u001b[0m Model 1 called with data  b'hey!'\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_one pid=76390)\u001b[0m Model 1 called with data  b'hey!'\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_two pid=76391)\u001b[0m Model 2 called with data  b'hey!'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:55:06,077 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 9.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:55:06,086 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 6.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:55:06,098 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 9.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:55:06,106 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 4.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=76360)\u001b[0m INFO 2022-12-27 09:55:06,113 http_proxy 127.0.0.1 http_proxy.py:361 - GET /composed 200 5.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_one pid=76390)\u001b[0m INFO 2022-12-27 09:55:06,074 model_one model_one#LQThHy replica.py:505 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_one pid=76390)\u001b[0m INFO 2022-12-27 09:55:06,083 model_one model_one#LQThHy replica.py:505 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_one pid=76390)\u001b[0m INFO 2022-12-27 09:55:06,092 model_one model_one#LQThHy replica.py:505 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_one pid=76390)\u001b[0m INFO 2022-12-27 09:55:06,104 model_one model_one#LQThHy replica.py:505 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_one pid=76390)\u001b[0m INFO 2022-12-27 09:55:06,111 model_one model_one#LQThHy replica.py:505 - HANDLE __call__ OK 0.1ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:model_two pid=76391)\u001b[0m INFO 2022-12-27 09:55:06,096 model_two model_two#cIlTlx replica.py:505 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=76395)\u001b[0m INFO 2022-12-27 09:55:06,075 ComposedModel ComposedModel#zxPfYB replica.py:505 - HANDLE __call__ OK 4.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=76395)\u001b[0m INFO 2022-12-27 09:55:06,084 ComposedModel ComposedModel#zxPfYB replica.py:505 - HANDLE __call__ OK 2.8ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=76395)\u001b[0m INFO 2022-12-27 09:55:06,097 ComposedModel ComposedModel#zxPfYB replica.py:505 - HANDLE __call__ OK 6.2ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=76395)\u001b[0m INFO 2022-12-27 09:55:06,104 ComposedModel ComposedModel#zxPfYB replica.py:505 - HANDLE __call__ OK 2.3ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:ComposedModel pid=76395)\u001b[0m INFO 2022-12-27 09:55:06,112 ComposedModel ComposedModel#zxPfYB replica.py:505 - HANDLE __call__ OK 2.4ms\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    resp = requests.get(\"http://127.0.0.1:8000/composed\", data=\"hey!\")\n",
    "    print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ebcd9-e5fa-428c-85e7-33e74cc96a9c",
   "metadata": {},
   "source": [
    "## Loading models\n",
    "\n",
    "Commonly, we want to load a model once per process, not on every request.\n",
    "\n",
    "This might be because the model is large/expensive to load, or we're retrieving it from a another system like a model registry or model database, and we want to minimize traffic against that other system while caching the model locally for performance.\n",
    "\n",
    "With RayServe, the pattern is to load/create the model in the service constructor (`__init__`), assign it to an instance variable, and then use that instance variable as needed for prediction. An example is shown at https://docs.ray.io/en/latest/serve/ml-models.html#integration-with-model-registries\n",
    "\n",
    "If you're loading very large models and want to improve performance further, there a few cool Ray tricks explained in this article, where the authors speed up loading a ~420MB flavor of the BERT language model: https://medium.com/ibm-data-ai/how-to-load-pytorch-models-340-times-faster-with-ray-8be751a6944c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467e469-9851-44c9-a6f8-680d291fdf4d",
   "metadata": {},
   "source": [
    "Just to pull back the curtain and take some of the magic away... what if our model didn't \"plug right in\" to a matching Predictor and Deployment class?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e3015-9ce6-4183-800e-e0ebedc51d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
